{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Verbal Explanation of Spatial Temporal GNNs for Traffic Forecasting</h1>\n",
    "    <h2>Monte Carlo Tree Search on the Metr-LA dataset</h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook the explanations of the predicted events of the *STGNN* on the *Metr-LA* dataset are computed by means of an *Explainer*. The explainer is an MCTS algorithm that performs a localized search on the input graph of the instance to be explained. It returns the important subgraph which serves as the explanation of the predicted selected event outputed by the STGNN. The search space is cut by a transparent *global heuristic* that exploits traffic domain-specific characteristics and limits the search on a subgraph of the input graph composed of the top scoring nodes according to the global heuristic. The explainer is inspired by the paper *Explaining temporal graph models through an explorer-navigator framework* <a name=\"cite_paper\"></a>[<sup>[1]</sup>](#note_paper).\n",
    "\n",
    "Firstly, grid search is applied to the training set in order to tune the hyperparameters of the explainer, afterwards, the important subgraphs of each dataset are extracted and the results are evaluated.\n",
    "\n",
    "For more detailed informations about the used functions, look into the corresponding docstrings inside the python files, inside the `src` folder.\n",
    "\n",
    "---\n",
    "<small>\n",
    "\n",
    "<a name=\"note_paper\"></a>[1]\n",
    "W. Xia et al. “Explaining temporal graph models through an explorer-navigator framework”. In: *The\n",
    "Eleventh International Conference on Learning Representations*, 2023.\n",
    "URL: https://openreview.net/forum?id=BR_ZhvcYbGJ.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6kbAULJKy-wu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HGIZtEY6y-wv"
   },
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "obBqbIuGy-wv"
   },
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHGuhgDmy-wv",
    "outputId": "7b633002-11e3-4114-dab9-e1f4216fcaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cpu\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhVgau7-y-ww"
   },
   "source": [
    "# 1 Loading the Data\n",
    "In this section the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HlTWno-1y-wx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BbiHBGWSy-wx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nb6d7kVZy-wx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula Madhavan\\AppData\\Local\\Temp\\ipykernel_20616\\2043237369.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n"
     ]
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the STGNN in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YIB5Kn57y-wx"
   },
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kwVU6H96y-wy"
   },
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x-lj4muty-wy"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the data scaler.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kNItiWwGy-wy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train.npy'))\n",
    "x_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_train_time.npy'))\n",
    "y_train_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_train_time.npy'))\n",
    "\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val.npy'))\n",
    "x_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_val_time.npy'))\n",
    "y_val_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_val_time.npy'))\n",
    "\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test.npy'))\n",
    "x_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'x_test_time.npy'))\n",
    "y_test_time = np.load(os.path.join(BASE_DATA_DIR, 'explainable', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Explainer\n",
    "In this section the explainer algorithm is applied through a grid search on the train dataset, then the results are evaluated.\n",
    "\n",
    "The explainer firstly computes a global heuristic $c_{v_i, v_j}^t$ among all input nodes $v_i$ at timesteps $t$ and the nodes of the event to explain $v_j$ as:\n",
    "$$ c_{v_i, v_j}^t = \\frac{\\Delta t}{\\Delta d} + \\frac{\\exp (- (\\Delta t + \\Delta d))}{\\Delta s} $$\n",
    "Where $\\Delta t$ is the temporal distance between $t$ and the last timestep of $v_j$, $\\Delta d$ their spatial distance and $\\Delta s$ the abdolute difference between the speed of $v_i$ at time $t$ and the average speed of $v_j$ in the event to explain.\n",
    "\n",
    "Afterwards, it performs a MCTS using the input subgraph composed of top scoring nodes by the heuristic as the root of the tree. It performs a series of rollout that involve choosing a path from the root to a leaf node. At each branching, a node of the graph described in the tree node is removed and a child tree node is obtained. The leaf node respects a sparsity threshold, as its graph has a limited pre-defined number of nodes. In practice, given node of the tree $\\mathcal{N}_i$:\n",
    "    \n",
    "1. A new tree node is expanded from $\\mathcal{N}_i$ according to an action. The action consists in removing a specific traffic node in the graph described in $\\mathcal{N}_i$.\n",
    "2. A child node is selected from $\\mathcal{N}_i$ among the expanded ones. In particular, it is selected the child node that maximizes exploration and exploitation.\n",
    "3. If $\\mathcal{N}_i$ is a leaf node, its reward is computed using the STGNN $\\Phi$.\n",
    "4. If $\\mathcal{N}_i$ is a leaf node, its reward is backpropagated from it up to the path nodes until the root is reached to update information useful for exploration and exploitation in the tree search.\n",
    "\n",
    "Ultimately, the final explanation outcome defined as the important subgraph is determined by the subgraph of the leaf node that attains the highest reward and which meets the specified sparsity threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spatial Distance Matrix\n",
    "Firstly, a speed distance matrix is built in order to compute the shortest distance in kms for each pair of nodes. This matrix is built by firstly finding the shortest path between couple of nodes through the distance matrix $A$ and then computing the length of the path in kms.\n",
    "\n",
    "Given a path $[n_1, ..., n_m]$, the distance among $n_1$ and $n_m$ is obtained as:\n",
    "$$ \\sum_{i}^{m -1} \\text{geodesic}(n_i, n_{i+1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bCEWyqCvy-wz"
   },
   "outputs": [],
   "source": [
    "from src.data.data_processing import get_distance_matrix\n",
    "\n",
    "if not os.path.exists(\n",
    "    os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy')):\n",
    "    # Build the distance matrix between the nodes.\n",
    "    distance_matrix = get_distance_matrix(\n",
    "        adj_matrix,\n",
    "        locations_df,\n",
    "        node_pos_dict)\n",
    "\n",
    "    # Save the distance matrix.\n",
    "    np.save(os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'),\n",
    "            distance_matrix)\n",
    "\n",
    "else:\n",
    "    # Load the distance matrix.\n",
    "    distance_matrix = np.load(\n",
    "        os.path.join(BASE_DATA_DIR, 'processed', 'distance_matrix.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Grid Search\n",
    "\n",
    "Grid search is performed on a portion of the train dataset in order to find the best set of hyperparameters of the Explainer:\n",
    "* `explanation size factor`: multiplied by the number of the nodes of the event to explain, defines the size of the important subgraph found by the explainer;\n",
    "* `cut size factor`: multiplied by `explanation size factor`, defines the number of top scoring nodes of the input graph according to the global heuristic that have to be used to cut the search;\n",
    "* `exploration weight`: parameter to weight the exploration in the MCTS;\n",
    "* `n rollouts`: the number of simulations to perform in the MCTS.\n",
    "\n",
    "The results are evaluated in terms of *Fidelity*$^-$ and *average time* to output the subgraph.\n",
    "\n",
    "* **Fidelity**$^\\textbf{-}$\n",
    "    $$\n",
    "        \\text{Fidelity}^- = \\frac{1}{N}  \\sum_{i=1}^N (\\Phi(\\mathcal{G}_i) - \\Phi({\\mathcal{G}}_i^m))\n",
    "    $$\n",
    "    with $N$ the number of instances, $\\Phi$ the STGNN, $\\mathcal{G}_i$ the $i^th$ input graph and $\\mathcal{G}_i^m$ the important subgraph.\n",
    "    The error $\\Phi(\\mathcal{G}_i) - \\Phi({\\mathcal{G}}_i^m)$ is computed using *MAE*, *RMSE* and *MAPE*.\n",
    "\n",
    "* **Average time:**\n",
    "    It measures the average time employed to output the important subgraph by for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKZtVQc2y-wz",
    "outputId": "55f982f6-ef10-46e5-b647-624f6f42ed31"
   },
   "outputs": [],
   "source": [
    "# from src.explanation.monte_carlo.explanation import apply_grid_search\n",
    "\n",
    "# apply_grid_search(\n",
    "#    x_train[::10],\n",
    "#    y_train[::10],\n",
    "#    distance_matrix,\n",
    "#    spatial_temporal_gnn,\n",
    "#    scaler,\n",
    " #   n_rollouts_list=[30, 50],\n",
    " #   explanation_size_factor_list=[2, 3],\n",
    "#    cut_size_factor_list=[2],\n",
    "#    exploration_weight_list=[5, 10, 20],\n",
    "#    remove_value_list=[0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters are expressed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qVtOLnQ68GDf"
   },
   "outputs": [],
   "source": [
    "CUT_SIZE_FACTOR = 2\n",
    "EXPLANATION_SIZE_FACTOR = 2\n",
    "EXPLORATION_WEIGHT = 20\n",
    "N_ROLLOUTS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Computing the explanations and Saving the Results\n",
    "In this section the explanations of the predicted events of the train, validation and test datasets are computed using the selected hyperparameters.\n",
    "\n",
    "The results are evaluated in terms of *Fidelity*$^-$ using *MAE*, *RMSE* and *MAPE*; *Average time*; *Fidelity*$^+$ using *MAE*, *RMSE* and *MAPE*; and *Sparsity*.\n",
    "\n",
    "*Fidelity*$^-$ and *Average time* have already been introduced. For what concerns *Fidelity*$^+$ and *Sparsity*:\n",
    "\n",
    "* **Fidelity**$^\\textbf{+}$\n",
    "    $$\n",
    "        \\text{Fidelity}^+ = \\frac{1}{N}  \\sum_{i=1}^N (\\Phi(\\mathcal{G}_i) - \\Phi({\\mathcal{G}}_i^{1 - m}))\n",
    "    $$\n",
    "    with $N$ the number of instances, $\\Phi$ the STGNN, $\\mathcal{G}_i$ the $i^th$ input graph and $\\mathcal{G}_i^{1 - m}$ the complement of the important subgraph.\n",
    "* **Sparsity:**\n",
    "    $$\\text{Sparsity} = \\frac{1}{N} \\sum_{i=1}^N (1 -\\frac{m_i}{M_i})$$\n",
    "    with $N$ the number of instances, $m_i$ the number of nodes in the important subgraph used for the explanation at instance $i$ and $M_i$ the number of nodes of the input graph at instance $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3FYN5mEu8GAl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "EXPLAINED_DATA_DIR = os.path.join(BASE_DATA_DIR, 'explained')\n",
    "os.makedirs(EXPLAINED_DATA_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdB__0mW8F-E",
    "outputId": "26b2c9fd-457f-4360-9be7-2cc98c677b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the explanations for the training set...\n",
      "[5/999] - 86s - MAE: { severe_congestion 2.9 -congestion N/D -free_flow N/D - total: 2.39 } - RMSE: { severe_congestion 4.01 -congestion N/D -free_flow N/D - total: 3.25 } - MAPE: { severe_congestion 10.5% -congestion N/D% -free_flow N/D% - total: 8.7% } - Average time: 17.3s                "
     ]
    }
   ],
   "source": [
    "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
    "\n",
    "\n",
    "print('Computing the explanations for the training set...')\n",
    "x_train_explained, y_train_explained, train_scores = get_all_explanations(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    distance_matrix,\n",
    "    spatial_temporal_gnn,\n",
    "    scaler,\n",
    "    n_rollouts=N_ROLLOUTS,\n",
    "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
    "    cut_size_factor=CUT_SIZE_FACTOR,\n",
    "    exploration_weight=EXPLORATION_WEIGHT,\n",
    "    remove_value=0.,\n",
    "    divide_by_traffic_cluster_kind=True)\n",
    "\n",
    "# Save the explained data.\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train.npy'), x_train_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train.npy'), y_train_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'train_scores.npy'), train_scores)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_train_time.npy'), x_train_time)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_train_time.npy'), y_train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdIyprvn8F7d",
    "outputId": "1eae176b-aa88-47dd-e9a9-76f5380e49ea"
   },
   "outputs": [],
   "source": [
    "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
    "\n",
    "\n",
    "print('Computing the explanations for the validation set...')\n",
    "x_val_explained, y_val_explained, val_scores = get_all_explanations(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    distance_matrix,\n",
    "    spatial_temporal_gnn,\n",
    "    scaler,\n",
    "    n_rollouts=N_ROLLOUTS,\n",
    "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
    "    cut_size_factor=CUT_SIZE_FACTOR,\n",
    "    exploration_weight=EXPLORATION_WEIGHT,\n",
    "    remove_value=0.,\n",
    "    divide_by_traffic_cluster_kind=True)\n",
    "\n",
    "# Save the explained data.\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val.npy'), x_val_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val.npy'), y_val_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'val_scores.npy'), val_scores)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_val_time.npy'), x_val_time)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_val_time.npy'), y_val_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-U5tti38F4V",
    "outputId": "d1e55598-33e1-47a2-b411-aee576e9ab57"
   },
   "outputs": [],
   "source": [
    "from src.explanation.monte_carlo.explanation import get_all_explanations\n",
    "\n",
    "\n",
    "print('Computing the explanations for the test set...')\n",
    "x_test_explained, y_test_explained, test_scores = get_all_explanations(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    distance_matrix,\n",
    "    spatial_temporal_gnn,\n",
    "    scaler,\n",
    "    n_rollouts=N_ROLLOUTS,\n",
    "    explanation_size_factor=EXPLANATION_SIZE_FACTOR,\n",
    "    cut_size_factor=CUT_SIZE_FACTOR,\n",
    "    exploration_weight=EXPLORATION_WEIGHT,\n",
    "    remove_value=0.,\n",
    "    divide_by_traffic_cluster_kind=True)\n",
    "\n",
    "# Save the explained data.\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test.npy'), x_test_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test.npy'), y_test_explained)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'test_scores.npy'), test_scores)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'x_test_time.npy'), x_test_time)\n",
    "np.save(os.path.join(EXPLAINED_DATA_DIR, 'y_test_time.npy'), y_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train.npy'))\n",
    "\n",
    "x_val_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val.npy'))\n",
    "\n",
    "x_test_explained = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.monte_carlo.explanation import print_all_fidelity_plus\n",
    "\n",
    "\n",
    "print('Computing the fidelity+ for the training set...')\n",
    "print_all_fidelity_plus(x_train, y_train, x_train_explained, spatial_temporal_gnn, scaler, remove_value=0.)\n",
    "print()\n",
    "\n",
    "print('Computing the fidelity+ for the validation set...')\n",
    "print_all_fidelity_plus(x_val, y_val, x_val_explained, spatial_temporal_gnn, scaler, remove_value=0.)\n",
    "print()\n",
    "\n",
    "print('Computing the fidelity+ for the test set...')\n",
    "print_all_fidelity_plus(x_test, y_test, x_test_explained, spatial_temporal_gnn, scaler, remove_value=0.)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explanation.monte_carlo.explanation import print_all_sparsity\n",
    "\n",
    "\n",
    "print('Computing the sparsity for the training set...')\n",
    "print_all_sparsity(x_train, x_train_explained)\n",
    "print()\n",
    "\n",
    "print('Computing the sparsity for the validation set...')\n",
    "print_all_sparsity(x_val, x_val_explained)\n",
    "print()\n",
    "\n",
    "print('Computing the sparsity for the test set...')\n",
    "print_all_sparsity(x_test, x_test_explained)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
