{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Verbal Explanation of Spatial Temporal GNNs for Traffic Forecasting</h1>\n",
    "    <h2>Verbal Explanations on the Metr-LA Dataset</h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "The verbal translation occurs through a template-based approach. This method involves substituting placeholders in textual templates with the chosen content to form coherent narratives.\n",
    "\n",
    "The verbal translation consists of composing a series of paragraphs by exploiting the content extracted from the graphical explanations. The first paragraph describes the predicted event and briefly sums up its causes, while the second to last paragraphs illustrate in detail each cause leading to the event which is each cluster of the important subgraph. The paragraphs describing the causes are sorted by the time they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Loading the Data\n",
    "In this section the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "_, node_ids_dict, _ = adj_matrix_structure\n",
    "\n",
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Get the node street and kilometrage dictionary.\n",
    "with open(os.path.join(BASE_DATA_DIR, 'structured', 'node_locations.pkl'), 'rb') as f:\n",
    "    node_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the explained data.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train.npy'))[..., :1]\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_train.npy'))[..., :1]\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val.npy'))[..., :1]\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_val.npy'))[..., :1]\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test.npy'))[..., :1]\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test.npy'))[..., :1]\n",
    "\n",
    "# Get the clustered data.\n",
    "x_train_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_train.npy'))\n",
    "x_val_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_val.npy'))\n",
    "x_test_clusters = np.load(os.path.join(BASE_DATA_DIR, 'clustered', 'x_test.npy'))\n",
    "\n",
    "\n",
    "# Get the time information of the explained data.\n",
    "x_train_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_train_time.npy'))\n",
    "y_train_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_train_time.npy'))\n",
    "x_val_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_val_time.npy'))\n",
    "y_val_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_val_time.npy'))\n",
    "x_test_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'x_test_time.npy'))\n",
    "y_test_times = np.load(os.path.join(BASE_DATA_DIR, 'explained', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are turned into km/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_train = x_train * MPH_TO_KMH_FACTOR\n",
    "y_train = y_train * MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_val = x_val * MPH_TO_KMH_FACTOR\n",
    "y_val = y_val * MPH_TO_KMH_FACTOR\n",
    "\n",
    "x_test = x_test * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Verbal Translation\n",
    "The translation is performed on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBAL_TRANSLATION_DIR = os.path.join(BASE_DATA_DIR, 'translated')\n",
    "\n",
    "os.makedirs(VERBAL_TRANSLATION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAG Model is initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa05737519434f68a1cfcf5e157aa611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.rag.rag_model import RAGModel\n",
    "\n",
    "# Load raw documents for RAG\n",
    "with open(os.path.join(BASE_DATA_DIR, 'raw', 'geo_data.txt'), 'r') as f:\n",
    "    raw_lines = f.readlines()\n",
    "    documents = [line.strip() for line in raw_lines]\n",
    "\n",
    "# Initialize the RAG model\n",
    "rag_model = RAGModel(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "from src.verbal_explanations.verbal_translation import translate_dataset\n",
    "\n",
    "train_translated = translate_dataset(\n",
    "    x_train,\n",
    "    x_train_times,\n",
    "    x_train_clusters,\n",
    "    y_train,\n",
    "    y_train_times,\n",
    "    node_pos_dict,\n",
    "    node_info,\n",
    "    rag_model)\n",
    "    \n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'train.npy'), train_translated)\n",
    "\n",
    "val_translated = translate_dataset(\n",
    "    x_val,\n",
    "    x_val_times,\n",
    "    x_val_clusters,\n",
    "    y_val,\n",
    "    y_val_times,\n",
    "    node_pos_dict,\n",
    "    node_info,\n",
    "    rag_model)\n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'val.npy'), val_translated)\n",
    "\n",
    "test_translated = translate_dataset(\n",
    "    x_test,\n",
    "    x_test_times,\n",
    "    x_test_clusters,\n",
    "    y_test,\n",
    "    y_test_times,\n",
    "    node_pos_dict,\n",
    "    node_info,\n",
    "    rag_model)\n",
    "\n",
    "np.save(os.path.join(VERBAL_TRANSLATION_DIR, 'test.npy'), test_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of a verbal translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A severe congestion was expected to occur on Glendale Freeway at kms 10 and 11 on Wednesday, 04/06/2012, from 07:45 to 08:40, with an average speed of 40.05 km/h. This was induced by a congestion and a free flow.\n",
      "\n",
      "Firstly, a contributing free flow occurred on Glendale Freeway at kms 8, 9, 10 and 11, with an average speed of 102.17 km/h, from 06:45 to 07:40. The free flow extended on Golden State Freeway at km 7.\n",
      "\n",
      "Lastly, a contributing severe congestion happened, averaging at a speed of 40.04 km/h, on, another time, Glendale Freeway at kms 9 and 10 from 07:10 to 07:40.\n",
      "\n",
      "Glendale Freeway (SR-2) often experiences congestion due to merging issues\n"
     ]
    }
   ],
   "source": [
    "print(test_translated[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
