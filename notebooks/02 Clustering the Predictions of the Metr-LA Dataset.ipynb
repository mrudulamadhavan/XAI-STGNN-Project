{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Verbal Explanation of Spatial Temporal GNNs for Traffic Forecasting</h1>\n",
    "    <h2>Clustering the Predictions of the Metr-LA dataset</h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook the predictions of the *STGNN* on the *Metr-LA* dataset are clustered in order to obtain events to explain such as congestions or free-flows.\n",
    "\n",
    "Firstly, a distance matrix is obtained for each predicted instance of the datasets in order to compute the spatio-temporal and speed distance among all nodes in the prediction. The distance matrix is obtained throught the method explained in *Revealing the day-to-day regularity of urban congestion patterns with\n",
    "3d speed maps* <a name=\"cite_paper\"></a>[<sup>[1]</sup>](#note_paper)\n",
    "\n",
    "Finally, the predictions are clustered through *DBSCAN*, while considering the distance matrix as a dissimilarity measure.\n",
    "\n",
    "For more detailed informations about the used functions, look into the corresponding docstrings inside the python files, inside the `src` folder.\n",
    "\n",
    "---\n",
    "<small>\n",
    "\n",
    "<a name=\"note_paper\"></a>[1] \n",
    "C. Lopez et al. “Revealing the day-to-day regularity of urban congestion patterns with\n",
    "3d speed maps”. In: *Scientific Reports, 7(1):14029*, September 2017. ISSN:\n",
    "2045-2322. DOI: 10.1038/s41598-017-14237-8. URL: https://doi.org/10.1038/s41598-017-14237-8.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vLMMte_HKFqA"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the main path in the root folder of the project.\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HnmplGx2KFqD"
   },
   "outputs": [],
   "source": [
    "# Settings for autoreloading.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVThP1k1KFqD"
   },
   "outputs": [],
   "source": [
    "from src.utils.seed import set_random_seed\n",
    "\n",
    "# Set the random seed for deterministic operations.\n",
    "SEED = 42\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9lFKnFUKFqE",
    "outputId": "325c9a3f-fb58-4e6f-a50d-53b544763d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected device is: \"cpu\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device for training and querying the model.\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'The selected device is: \"{DEVICE}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkJzAnB5KFqF"
   },
   "source": [
    "# 1 Loading the Data\n",
    "In this section the data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sLOMzmxnKFqG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DATA_DIR = os.path.join('..', 'data', 'metr-la')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7xeJlGK_KFqG"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(BASE_DATA_DIR, 'processed', 'scaler.pkl'), 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q9OqSPOJKFqH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula Madhavan\\AppData\\Local\\Temp\\ipykernel_5880\\3998463308.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n"
     ]
    }
   ],
   "source": [
    "from src.spatial_temporal_gnn.model import SpatialTemporalGNN\n",
    "from src.data.data_extraction import get_adjacency_matrix\n",
    "\n",
    "# Get the adjacency matrix\n",
    "adj_matrix_structure = get_adjacency_matrix(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'adj_mx_metr_la.pkl'))\n",
    "\n",
    "# Get the header of the adjacency matrix, the node indices and the\n",
    "# matrix itself.\n",
    "header, node_ids_dict, adj_matrix = adj_matrix_structure\n",
    "\n",
    "# Get the STGNN and load the checkpoints.\n",
    "spatial_temporal_gnn = SpatialTemporalGNN(9, 1, 12, 12, adj_matrix, DEVICE, 64)\n",
    "\n",
    "stgnn_checkpoints_path = os.path.join('..', 'models', 'checkpoints',\n",
    "                                      'st_gnn_metr_la.pth')\n",
    "\n",
    "stgnn_checkpoints = torch.load(stgnn_checkpoints_path)\n",
    "spatial_temporal_gnn.load_state_dict(stgnn_checkpoints['model_state_dict'])\n",
    "\n",
    "# Set the model in evaluation mode.\n",
    "spatial_temporal_gnn.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V_2BKaDPKFqH"
   },
   "outputs": [],
   "source": [
    "from src.data.data_extraction import get_locations_dataframe\n",
    "\n",
    "# Get the dataframe containing the latitude and longitude of each sensor.\n",
    "locations_df = get_locations_dataframe(\n",
    "    os.path.join(BASE_DATA_DIR, 'raw', 'graph_sensor_locations_metr_la.csv'),\n",
    "    has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eiWj_ujNKFqI"
   },
   "outputs": [],
   "source": [
    "# Get the node positions dictionary.\n",
    "node_pos_dict = { i: id for id, i in node_ids_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ioKQfKUrKFqI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from src.spatial_temporal_gnn.prediction import predict\n",
    "\n",
    "# Get the data and the values predicted by the STGNN.\n",
    "x_train = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_train.npy'))\n",
    "y_train = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_train.npy'))\n",
    "x_val = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_val.npy'))\n",
    "y_val = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_val.npy'))\n",
    "x_test = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(BASE_DATA_DIR, 'predicted', 'y_test.npy'))\n",
    "\n",
    "# Get the time information of the train, validation and test sets.\n",
    "x_train_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'x_train_time.npy'))\n",
    "y_train_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'y_train_time.npy'))\n",
    "x_val_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'x_val_time.npy'))\n",
    "y_val_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'y_val_time.npy'))\n",
    "x_test_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'x_test_time.npy'))\n",
    "y_test_time = np.load(\n",
    "    os.path.join(BASE_DATA_DIR, 'predicted', 'y_test_time.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are turned into km/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7277uo5lKFqI"
   },
   "outputs": [],
   "source": [
    "# Turn the results in kilometers per hour.\n",
    "from src.utils.config import MPH_TO_KMH_FACTOR\n",
    "\n",
    "\n",
    "y_train = y_train * MPH_TO_KMH_FACTOR\n",
    "y_val = y_val * MPH_TO_KMH_FACTOR\n",
    "y_test = y_test * MPH_TO_KMH_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EE1HKIB6KFqI"
   },
   "outputs": [],
   "source": [
    "_, n_timesteps, n_nodes, _ = y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Distance matrix\n",
    "The distance matrix $M$ used as a metric of dissimilarity in DBSCAN is computed separately for each instance and is composed of:\n",
    "* A *spatial distance matrix* $M_d$;\n",
    "* A *temporal distance matrix* $M_t$;\n",
    "* A *speed distance matrix* $M_s$\n",
    "\n",
    "The matrices $M_d$, $M_t$ and $M_s$ are each scaled through *min-max scaling* between $0$ and $1$ and summed together in order to obtain $M$ as follows: \n",
    "$$M = W \\cdot M_s + M_d + M_t$$\n",
    "where the speed distance is overweighted by multiplying $M_s$ by a factor $W \\geq 1$ because the speed variable is expected to play a predominant role during the clustering process. $M$ is next normalized again between $0$ and $1$ through min-max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcmu0JKxKFqJ"
   },
   "source": [
    "## 2.1 Spatial Distance Matrix\n",
    "The *spatial distance matrix* $M_d$ is an $(N \\cdot T') \\times (N \\cdot T')$ matrix derived from the adjacency matrix of the traffic network $A \\in \\mathbb{R}^{N \\times N}$ and describing the spatial distant of each nodes regardless of their timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZYeghX61KFqJ"
   },
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_adjacency_distance_matrix)\n",
    "\n",
    "adj_distance_matrix = get_adjacency_distance_matrix(adj_matrix, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-8kWCYYKFqJ",
    "outputId": "43f04230-a150-47c0-b461-4018695d2b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Adjacency Distance Matrix: (2484, 2484)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the Adjacency Distance Matrix: {adj_distance_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGI6lnJZKFqJ"
   },
   "source": [
    "# 2.2 Temporal Distance Matrix\n",
    "The *temporal distance matrix* $M_t$ is an $(N \\cdot T') \\times (N \\cdot T')$ matrix describing the temporal distance of the nodes at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CrwG-PfaKFqJ"
   },
   "outputs": [],
   "source": [
    "from src.explanation.clustering.clustering import (\n",
    "    get_temporal_distance_matrix)\n",
    "\n",
    "temporal_distance_matrix = get_temporal_distance_matrix(n_nodes, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7dSerYZKFqJ",
    "outputId": "690b58be-692c-45f1-9320-157a0b905c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Temporal Distance Matrix: (2484, 2484)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the Temporal Distance Matrix:',\n",
    "      f'{temporal_distance_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Speed Distance Matrix\n",
    "The *speed distance matrix* $M_s$ is an $(N \\cdot T') \\times (N \\cdot T')$ matrix describing the speed distance of the nodes at each timestep.\n",
    "\n",
    "It is computed for each instance separately, before DBSCAN is performed. The value $W$, overweighting it is set as $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggPKt03fKFqK"
   },
   "source": [
    "# 3 Clustering Function\n",
    "Next, the clustering technique *DBSCAN* is used on the distance matrix $M$. It identifies whether each node of the predicted traffic network at a given time, specified by an index in $M$, is part of a distinct traffic cluster or categorized as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Grid Search\n",
    "Grid search is performed on a portion of the train dataset in order to find the best set of hyperparameters of DBSCAN:\n",
    "* `eps`\n",
    "* `min_samples`\n",
    "\n",
    "The results are evaluated in terms of *Within Cluster Variance*, *Cluster Dissimilarity* and *noise ratio*.\n",
    "\n",
    "* **Within Cluster Variance:**\n",
    "    $$WCV = \\frac{1}{\\sum_{i = 1}^N n_i} \\frac{\\sum_{i = 1}^N n_i \\sigma^2_i}{\\sigma^2} $$\n",
    "    with $N$ the number of clusters, $n_i$ the nodes of the $i^{th}$ cluster in the predicted network and $\\sigma_i^2$ the speed variance among its nodes. $\\sigma^2$ is the variance among all nodes of the prediction.\n",
    "\n",
    "* **Cluster Dissimilarity:**\n",
    "    $$ CD = \\frac{\\sum_{i = 1}^N \\sum_{k = i + 1}^N \\sqrt{n_i \\cdot n_k} \\cdot |\\mu_i - \\mu_k|}{\\sum_{i = 1}^N \\sum_{k = i + 1}^N \\sqrt{n_i \\cdot n_k}} $$\n",
    "    with $\\mu_i$ the mean speed value of cluster $i$.\n",
    "\n",
    "* **Noise Ratio:** It measures the ratio of the nodes classified as outliers by DBSCAN in a predicted traffic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLiV_PdkKFqK",
    "outputId": "be7b28cc-9a33-4374-f5ac-28d6166b2aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.1 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 5.96 Noise points ratio: 0.997\n",
      "\n",
      "eps: 0.1 min_samples: 7\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.1 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.945 Connected Cluster Dissimilarity: 12 Noise points ratio: 0.918\n",
      "\n",
      "eps: 0.15 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.994 Connected Cluster Dissimilarity: 7.76 Noise points ratio: 0.989\n",
      "\n",
      "eps: 0.15 min_samples: 10\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0.179 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 12\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.15 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.454 Connected Cluster Dissimilarity: 12 Noise points ratio: 0.319\n",
      "\n",
      "eps: 0.2 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.891 Connected Cluster Dissimilarity: 10.9 Noise points ratio: 0.828\n",
      "\n",
      "eps: 0.2 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.14 Noise points ratio: 0.982\n",
      "\n",
      "eps: 0.2 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.42 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.2 min_samples: 15\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 17\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.2 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.25 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0902 Connected Cluster Dissimilarity: 14.9 Noise points ratio: 0.0643\n",
      "\n",
      "eps: 0.25 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.723 Connected Cluster Dissimilarity: 12.5 Noise points ratio: 0.644\n",
      "\n",
      "eps: 0.25 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.945 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.905\n",
      "\n",
      "eps: 0.25 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.985 Connected Cluster Dissimilarity: 10.2 Noise points ratio: 0.973\n",
      "\n",
      "eps: 0.25 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.997 Connected Cluster Dissimilarity: 4.66 Noise points ratio: 0.994\n",
      "\n",
      "eps: 0.25 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 0.471 Noise points ratio: 0.999\n",
      "\n",
      "eps: 0.25 min_samples: 20\n",
      "\tWithin-Cluster Variance: 1 Connected Cluster Dissimilarity: 0 Noise points ratio: 1\n",
      "\n",
      "eps: 0.3 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0511 Connected Cluster Dissimilarity: 15.2 Noise points ratio: 0.0268\n",
      "\n",
      "eps: 0.3 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.244 Connected Cluster Dissimilarity: 13.1 Noise points ratio: 0.163\n",
      "\n",
      "eps: 0.3 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.743 Connected Cluster Dissimilarity: 11.4 Noise points ratio: 0.629\n",
      "\n",
      "eps: 0.3 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.898 Connected Cluster Dissimilarity: 9.98 Noise points ratio: 0.814\n",
      "\n",
      "eps: 0.3 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.978 Connected Cluster Dissimilarity: 10.5 Noise points ratio: 0.958\n",
      "\n",
      "eps: 0.3 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.991 Connected Cluster Dissimilarity: 8.02 Noise points ratio: 0.981\n",
      "\n",
      "eps: 0.3 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.999 Connected Cluster Dissimilarity: 1.68 Noise points ratio: 0.998\n",
      "\n",
      "eps: 0.35 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0437 Connected Cluster Dissimilarity: 15.4 Noise points ratio: 0.0135\n",
      "\n",
      "eps: 0.35 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0847 Connected Cluster Dissimilarity: 15 Noise points ratio: 0.051\n",
      "\n",
      "eps: 0.35 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.592 Connected Cluster Dissimilarity: 12.7 Noise points ratio: 0.489\n",
      "\n",
      "eps: 0.35 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.75 Connected Cluster Dissimilarity: 11.6 Noise points ratio: 0.648\n",
      "\n",
      "eps: 0.35 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.938 Connected Cluster Dissimilarity: 11.2 Noise points ratio: 0.894\n",
      "\n",
      "eps: 0.35 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.969 Connected Cluster Dissimilarity: 10.8 Noise points ratio: 0.942\n",
      "\n",
      "eps: 0.35 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.992 Connected Cluster Dissimilarity: 7.53 Noise points ratio: 0.983\n",
      "\n",
      "eps: 0.4 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.047 Connected Cluster Dissimilarity: 15.4 Noise points ratio: 0.00805\n",
      "\n",
      "eps: 0.4 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0623 Connected Cluster Dissimilarity: 15.5 Noise points ratio: 0.0226\n",
      "\n",
      "eps: 0.4 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.457 Connected Cluster Dissimilarity: 13.1 Noise points ratio: 0.335\n",
      "\n",
      "eps: 0.4 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.557 Connected Cluster Dissimilarity: 12.5 Noise points ratio: 0.434\n",
      "\n",
      "eps: 0.4 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.779 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.66\n",
      "\n",
      "eps: 0.4 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.892 Connected Cluster Dissimilarity: 10.7 Noise points ratio: 0.81\n",
      "\n",
      "eps: 0.4 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.96 Connected Cluster Dissimilarity: 10.5 Noise points ratio: 0.922\n",
      "\n",
      "eps: 0.45 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0563 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.00462\n",
      "\n",
      "eps: 0.45 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0641 Connected Cluster Dissimilarity: 15.7 Noise points ratio: 0.0124\n",
      "\n",
      "eps: 0.45 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.394 Connected Cluster Dissimilarity: 13.6 Noise points ratio: 0.263\n",
      "\n",
      "eps: 0.45 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.463 Connected Cluster Dissimilarity: 13.1 Noise points ratio: 0.343\n",
      "\n",
      "eps: 0.45 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.637 Connected Cluster Dissimilarity: 12.2 Noise points ratio: 0.513\n",
      "\n",
      "eps: 0.45 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.782 Connected Cluster Dissimilarity: 11.6 Noise points ratio: 0.67\n",
      "\n",
      "eps: 0.45 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.906 Connected Cluster Dissimilarity: 11.1 Noise points ratio: 0.84\n",
      "\n",
      "eps: 0.5 min_samples: 5\n",
      "\tWithin-Cluster Variance: 0.0718 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.00319\n",
      "\n",
      "eps: 0.5 min_samples: 7\n",
      "\tWithin-Cluster Variance: 0.0753 Connected Cluster Dissimilarity: 16 Noise points ratio: 0.00736\n",
      "\n",
      "eps: 0.5 min_samples: 10\n",
      "\tWithin-Cluster Variance: 0.102 Connected Cluster Dissimilarity: 15.4 Noise points ratio: 0.0415\n",
      "\n",
      "eps: 0.5 min_samples: 12\n",
      "\tWithin-Cluster Variance: 0.384 Connected Cluster Dissimilarity: 13.6 Noise points ratio: 0.238\n",
      "\n",
      "eps: 0.5 min_samples: 15\n",
      "\tWithin-Cluster Variance: 0.49 Connected Cluster Dissimilarity: 12.8 Noise points ratio: 0.351\n",
      "\n",
      "eps: 0.5 min_samples: 17\n",
      "\tWithin-Cluster Variance: 0.594 Connected Cluster Dissimilarity: 12.1 Noise points ratio: 0.443\n",
      "\n",
      "eps: 0.5 min_samples: 20\n",
      "\tWithin-Cluster Variance: 0.774 Connected Cluster Dissimilarity: 10.9 Noise points ratio: 0.628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.evaluation import apply_grid_search\n",
    "\n",
    "# Apply the grid search on a subset of the training set.\n",
    "apply_grid_search(\n",
    "    instances=y_train[:200],\n",
    "    eps_list=[.1, .15, .2, .25, .3, .35, .4, .45, .5],\n",
    "    min_samples_list=[5, 7, 10, 12, 15, 17, 20],\n",
    "    adj_distance_matrix=adj_distance_matrix,\n",
    "    temporal_distance_matrix=temporal_distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters are expressed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uaBIIuX6KFqK"
   },
   "outputs": [],
   "source": [
    "# Set the best parameters based on the results of the grid search.\n",
    "\n",
    "EPS = .35\n",
    "MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Clustering and Saving the Results\n",
    "In this section the clustering of the predictions of the train, validation and test datasets is computed using the selected hyperparameters.\n",
    "\n",
    "The results are evaluated in terms of *Within Cluster Variance*, *Cluster Dissimilarity* and *noise ratio* on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vB3ggcBsKFqK",
    "outputId": "6d93cfe5-cec5-4a5f-de86-fcf8e78b4c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-Cluster Variance on the test set: 0.0708 Connected Cluster Dissimilarity on the test set: 15 Noise points ratio on the test set: 0.033\n"
     ]
    }
   ],
   "source": [
    "from src.explanation.clustering.evaluation import get_dataset_clustering_scores\n",
    "\n",
    "(avg_within_cluster_variance, avg_connected_cluster_dissimilarity,\n",
    " avg_noise_ratio) = get_dataset_clustering_scores(\n",
    "     y_test, adj_distance_matrix, temporal_distance_matrix, EPS, MIN_SAMPLES)\n",
    "\n",
    "print(\n",
    "    'Within-Cluster Variance on the test set:',\n",
    "    f'{avg_within_cluster_variance:.3g}',\n",
    "    'Connected Cluster Dissimilarity on the test set:',\n",
    "    f'{avg_connected_cluster_dissimilarity:.3g}',\n",
    "    'Noise points ratio on the test set:', f'{avg_noise_ratio:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vfSDHmz_KFqL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join('..', 'data', 'metr-la', 'explainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MwxzqFElKFqM"
   },
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from src.explanation.clustering.clustering import (\n",
    "    get_dataset_for_explainability)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "(x_train_expl, y_train_expl,\n",
    " x_train_time_expl, y_train_time_expl) = get_dataset_for_explainability(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_train_time,\n",
    "    y_train_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=1_000)\n",
    "save(os.path.join(DATA_DIR, 'x_train.npy'), x_train_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_train.npy'), y_train_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_train_time.npy'), x_train_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_train_time.npy'), y_train_time_expl)\n",
    "\n",
    "(x_val_expl, y_val_expl,\n",
    " x_val_time_expl, y_val_time_expl) = get_dataset_for_explainability(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_val_time,\n",
    "    y_val_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=200)\n",
    "save(os.path.join(DATA_DIR, 'x_val.npy'), x_val_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_val.npy'), y_val_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_val_time.npy'), x_val_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_val_time.npy'), y_val_time_expl)\n",
    "\n",
    "(x_test_expl, y_test_expl,\n",
    " x_test_time_expl, y_test_time_expl) = get_dataset_for_explainability(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    x_test_time,\n",
    "    y_test_time,\n",
    "    EPS,\n",
    "    MIN_SAMPLES,\n",
    "    adj_distance_matrix,\n",
    "    temporal_distance_matrix,\n",
    "    total_samples=300)\n",
    "save(os.path.join(DATA_DIR, 'x_test.npy'), x_test_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_test.npy'), y_test_expl)\n",
    "save(os.path.join(DATA_DIR, 'x_test_time.npy'), x_test_time_expl)\n",
    "save(os.path.join(DATA_DIR, 'y_test_time.npy'), y_test_time_expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvMGNnt0KFqM",
    "outputId": "4559a9c5-b8fb-409b-ad2f-ee4b01ae06d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset for explainability shapes: (999, 12, 207, 9) (999, 12, 207, 1)\n",
      "Validation dataset for explainability shapes: (198, 12, 207, 9) (198, 12, 207, 1)\n",
      "Test dataset for explainability shapes: (300, 12, 207, 9) (300, 12, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset for explainability shapes:',\n",
    "      x_train_expl.shape, y_train_expl.shape)\n",
    "print('Validation dataset for explainability shapes:',\n",
    "      x_val_expl.shape, y_val_expl.shape)\n",
    "print('Test dataset for explainability shapes:',\n",
    "      x_test_expl.shape, y_test_expl.shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
